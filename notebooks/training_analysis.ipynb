{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Analysis and Visualization\n",
    "\n",
    "This notebook provides analysis of model training results and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from config import Config\n",
    "from utils import plot_training_history\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "config = Config()\n",
    "history_path = config.LOGS_DIR / 'training_history.json'\n",
    "\n",
    "if history_path.exists():\n",
    "    with open(history_path, 'r') as f:\n",
    "        history_data = json.load(f)\n",
    "    print('Training history loaded successfully!')\n",
    "    print(f'Training epochs: {len(history_data["accuracy"])}')\n",
    "else:\n",
    "    print('No training history found. Please train a model first.')\n",
    "    history_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history_data:\n",
    "    # Create comprehensive training visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    epochs = range(1, len(history_data['accuracy']) + 1)\n",
    "    axes[0, 0].plot(epochs, history_data['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history_data['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 1].plot(epochs, history_data['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history_data['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning curve analysis\n",
    "    train_acc = np.array(history_data['accuracy'])\n",
    "    val_acc = np.array(history_data['val_accuracy'])\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    axes[1, 0].plot(epochs, gap, 'g-', linewidth=2)\n",
    "    axes[1, 0].set_title('Training-Validation Gap', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy Gap')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0.05, color='r', linestyle='--', alpha=0.5, label='5% Gap Threshold')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Performance summary\n",
    "    best_val_acc = max(history_data['val_accuracy'])\n",
    "    best_epoch = history_data['val_accuracy'].index(best_val_acc) + 1\n",
    "    final_val_acc = history_data['val_accuracy'][-1]\n",
    "    \n",
    "    summary_text = f'''Performance Summary:\n",
    "Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_epoch})\n",
    "Final Validation Accuracy: {final_val_acc:.4f}\n",
    "Total Epochs: {len(epochs)}\n",
    "Max Train-Val Gap: {max(gap):.4f}\n",
    "Final Train-Val Gap: {gap[-1]:.4f}'''\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results if available\n",
    "eval_path = config.LOGS_DIR / 'detailed_evaluation_results.json'\n",
    "\n",
    "if eval_path.exists():\n",
    "    with open(eval_path, 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "    \n",
    "    print('Model Evaluation Results:')\n",
    "    print(f'Test Accuracy: {eval_data["accuracy"]:.4f}')\n",
    "    print(f'Test Precision: {eval_data["precision"]:.4f}')\n",
    "    print(f'Test Recall: {eval_data["recall"]:.4f}')\n",
    "    print(f'Test F1-Score: {eval_data["f1_score"]:.4f}')\n",
    "    \n",
    "    # Visualize confusion matrix if available\n",
    "    if 'confusion_matrix' in eval_data:\n",
    "        cm = np.array(eval_data['confusion_matrix'])\n",
    "        class_names = eval_data['class_names']\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No evaluation results found. Please evaluate the model first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze model if available\n",
    "model_path = config.get_model_path()\n",
    "\n",
    "if model_path.exists():\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    print('Model Architecture Summary:')\n",
    "    model.summary()\n",
    "    \n",
    "    # Parameter analysis\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_params = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "    \n",
    "    print(f'\\nParameter Analysis:')\n",
    "    print(f'Total parameters: {total_params:,}')\n",
    "    print(f'Trainable parameters: {trainable_params:,}')\n",
    "    print(f'Non-trainable parameters: {non_trainable_params:,}')\n",
    "    \n",
    "    # Layer analysis\n",
    "    layer_types = {}\n",
    "    for layer in model.layers:\n",
    "        layer_type = type(layer).__name__\n",
    "        layer_types[layer_type] = layer_types.get(layer_type, 0) + 1\n",
    "    \n",
    "    print(f'\\nLayer Type Distribution:')\n",
    "    for layer_type, count in layer_types.items():\n",
    "        print(f'{layer_type}: {count}')\n",
    "        \n",
    "    # Visualize layer distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(layer_types.keys(), layer_types.values())\n",
    "    plt.title('Layer Type Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Layer Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No trained model found. Please train a model first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Analysis Complete!')\n",
    "print('=' * 50)\n",
    "\n",
    "if history_data:\n",
    "    final_acc = history_data['val_accuracy'][-1]\n",
    "    \n",
    "    if final_acc >= 0.98:\n",
    "        print('🎉 Excellent! Model achieved 98%+ accuracy target!')\n",
    "    elif final_acc >= 0.95:\n",
    "        print('🌟 Great performance! Model achieved 95%+ accuracy!')\n",
    "    elif final_acc >= 0.90:\n",
    "        print('👍 Good performance! Model achieved 90%+ accuracy!')\n",
    "    else:\n",
    "        print('📈 Model performance can be improved.')\n",
    "        \n",
    "    print('\\nRecommendations:')\n",
    "    \n",
    "    if gap[-1] > 0.05:\n",
    "        print('- Consider adding more regularization (dropout, weight decay)')\n",
    "        print('- Increase data augmentation')\n",
    "        print('- Collect more training data')\n",
    "    \n",
    "    if final_acc < 0.95:\n",
    "        print('- Try transfer learning with pre-trained models')\n",
    "        print('- Experiment with different architectures')\n",
    "        print('- Optimize hyperparameters')\n",
    "        \n",
    "    print('\\n📊 Check the logs directory for detailed training metrics!')\n",
    "    print('📈 Use TensorBoard for interactive visualization: tensorboard --logdir logs/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}